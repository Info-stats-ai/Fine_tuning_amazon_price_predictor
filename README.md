# Fine_tuning_amazon_price_predictor:
## Till Now:
Parameter-Efficient Fine-Tuning (PEFT) using QLoRA to adapt the Meta-Llama-3.1-8B model. It explores 8-bit and 4-bit quantization, showcasing significant memory reduction for large models. Key techniques include LoRA adapters and double quantization for efficient fine-tuning. The notebook calculates memory footprints and explains adapter architecture for better understanding.

